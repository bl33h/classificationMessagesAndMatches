{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio 2\n",
    "## Miembros del equipo\n",
    "- Ricardo Méndez 21289\n",
    "- Sara Echeverría 21371\n",
    "- Melissa Pérez 21385\n",
    "- Francisco Castillo 21562"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ¿Por qué el modelo de Naive Bayes se le considera “naive”?\n",
    "\n",
    "La teoría de Bayes asume que los eventos son dependientes entre sí. Al implementarlo con un acercamiento _naive_ implica que asumimos que los eventos son independientes entre sí. Donde, básicamente, describe que una característica de un conjunto de datos no está relacionada con otra de ellas. En otras palabras, es por su simplicidad de la independencia condicional entre las características o atributos que describen los datos. [(Shoba, 2018)](https://www.sciencedirect.com/science/article/abs/pii/S0169716118300191)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explique la formulación matemática que se busca optimizar en Support Vector Machine, además responda ¿cómo funciona el truco del Kernel para este modelo? \n",
    "Las máquinas de soporte vectorial buscan encontrar una frontera que maximice el margen de separation para la classification de los datos. La prediction de los datos se realiza con la siguiente formula:\n",
    "$$\n",
    "y_i = sign(w^Tx_i+b)\n",
    "$$\n",
    "El margen se optimiza a traves de la distancia perpendicular desde la frontera hacia al punto mas cercano de cada clase; es decir, maximizamos esta distancia. Esto se resuelve con multiplicadores de Lagrange (Tantos multiplicadores como cantidad de puntos tenemos)\n",
    "$$\n",
    "argmin_{w,ɑ} \\frac{1}{2}w^Tw- \\sum_{n,m=1}^{N}ɑ_mɑ_ny_my_nx_n,x_m\n",
    "$$\n",
    "Tras resolver el problema de optimización, se obtiene la siguiente formula:\n",
    "$$\n",
    "argmax_w \\sum_{n=1}^{N}ɑ_n - \\frac{1}{2} \\sum_{n,m=1}^{N}ɑ_nɑ_my_my_n{x_n}^Tx_m\n",
    "$$\n",
    "Se puede apreciar que ya no dependemos de los pesos ($w$) y el bias ($b$), sino que dependemos de los multiplicadores de Lagrange ($ɑ$). Estos multiplicadores son los que nos permiten encontrar la frontera de decision.\n",
    "\n",
    "Las predicciones ahora se hacen de la siguiente manera:\n",
    "$$\n",
    "y_i = sign(\\sum_{n=1}^{N}ɑ_ny_nx_n^Tx_i+b)\n",
    "$$\n",
    "Los datos de $x_i$ solamente aparecen como un producto punto, el cual podemos representar con una función de kernel.\n",
    "$$\n",
    "y_i = sign(\\sum_{n=1}^{N}ɑ_ny_nK(x_n,x_i)+b)\n",
    "$$\n",
    "\n",
    "EL utilizar esta function nos permite ingress el _truco del kernel_. Este truco nos permite mapear los datos a un espacio de mayor dimension, donde es más facial encontrar una frontera de decision que no es necesariamente lineal.  [(Ranjan, 2019)](https://towardsdatascience.com/truly-understanding-the-kernel-trick-1aeb11560769)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Qué tipo de ensemble learning es este modelo (Random Forest)?\n",
    "Random Forest es un ensemble learning de tipo _Bootsrap Aggregation_ o _bagging_. Se utiliza para reducir la varianza de un modelo de machine learning a base de crear multiples modelos y combinarlos para obtener un modelo final. En un _Random Forest_ todos los modelos son árboles de decision. Cada arbol se entrena con diferentes subconjuntos de datos los cuales son combinados para obtener un modelo final. [(Sruthi, 2024)](https://www.analyticsvidhya.com/blog/2021/06/understanding-random-forest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Cuál es la idea general detrás de Random Forest?\n",
    "Como se ha mencionado anteriormente, el objetivo es crear un modelo de aprendizaje que combine la salida de multiples arboles de decision para llegar a un resultado unico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ¿Por qué se busca baja correlación entre los árboles de Random Forest?\n",
    "Se busca que los _features_ tengan baja correlación para reducir la varianza y mejorar la precision de las predicciones. Si la correlación es alta entre estos arboles, es muy probable que cometan los mismos errores en la predicción, lo que no ofrecería ninguna mejora. [(Breiman & Cutler, s.f.)](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 Naive Bayes: Clasificador de Mensajes Ham/Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3 Clasificación de Partidas de League of Legends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.1 Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.2 Árboles de Decisión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3.3 Comparación"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
